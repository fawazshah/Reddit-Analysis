{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_sentiment_reddit.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1pI08n+HVm2eGm4Q0jT57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fawazshah/Reddit-Analysis/blob/main/4_sentiment_vocab_overlap_reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moh_1_l1S5wb",
        "outputId": "936fc1b7-1c6e-40f1-8ea0-23efc88e55e1"
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K-tm1qPRtJ2"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue_Be0bgRotx"
      },
      "source": [
        "submissions_lib_dem_con_rep_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/submissions_top300_year_liberal_democrats_conservative_republicans.tsv'\n",
        "submissions_lib_dem_con_rep_df = pd.read_csv(submissions_lib_dem_con_rep_url, sep='\\t')\n",
        "\n",
        "comments_lib_dem_con_rep_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/comments_top300_year_liberal_democrats_conservative_republicans.tsv'\n",
        "comments_lib_dem_con_rep_df = pd.read_csv(comments_lib_dem_con_rep_url, sep='\\t')\n",
        "\n",
        "submissions_ob_clin_sls_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/submissions_top300_year_obama_hillaryclinton_shitliberalssay.tsv'\n",
        "submissions_ob_clin_sls_df = pd.read_csv(submissions_ob_clin_sls_url, sep='\\t')\n",
        "\n",
        "comments_ob_clin_sls_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/comments_top300_year_obama_hillaryclinton_shitliberalssay.tsv'\n",
        "comments_ob_clin_sls_df = pd.read_csv(comments_ob_clin_sls_url, sep='\\t')\n",
        "\n",
        "submissions_libertarian_sfp_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/submissions_top300_year_libertarian_sandersforpresident.tsv'\n",
        "submissions_libertarian_sfp_df = pd.read_csv(submissions_libertarian_sfp_url, sep='\\t')\n",
        "\n",
        "comments_libertarian_sfp_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/assembled-data/comments_top300_year_libertarian_sandersforpresident.tsv'\n",
        "comments_libertarian_sfp_df = pd.read_csv(comments_libertarian_sfp_url, sep='\\t')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn-hh2zA3Zp9"
      },
      "source": [
        "submissions_df = pd.concat([submissions_lib_dem_con_rep_df, submissions_ob_clin_sls_df, submissions_libertarian_sfp_df], ignore_index=True)\n",
        "comments_df = pd.concat([comments_lib_dem_con_rep_df, comments_ob_clin_sls_df, comments_libertarian_sfp_df], ignore_index=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoPSpQi10wtD"
      },
      "source": [
        "### Data checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-mEhaxe0caL",
        "outputId": "2adbfc90-fc43-48c1-da43-5550f4f0b41f"
      },
      "source": [
        "print(submissions_df['article headline'].isna().sum())\n",
        "print(submissions_df['article body'].isna().sum())\n",
        "print(comments_df['comment body'].isna().sum())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G71r8x5LXUXM"
      },
      "source": [
        "comments_df.dropna(subset=['comment body'], inplace=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v79DxtdVyInl"
      },
      "source": [
        "### Fixing left/right class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFVqBt8TyLbw",
        "outputId": "fb0140e8-c83a-443a-dd4f-61ee76454637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(submissions_df['bias'].value_counts())\n",
        "print(comments_df['bias'].value_counts())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "left     605\n",
            "right    434\n",
            "Name: bias, dtype: int64\n",
            "right    79809\n",
            "left     22164\n",
            "Name: bias, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8rH_to01Evn"
      },
      "source": [
        "Right is minority class in articles, however left is minority class in comments. We can't remove class imbalance independently in articles and in comments, since for any article we want to make sure all its comments are still in the comments dataset. Thus we fix class imbalance in articles by trimming all classes down to the size of the minority class, and then removing all comments associated with the removed articles. Thus comments won't be exactly balanced, but hopefully will be close enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVeXziWu3FMr",
        "outputId": "767b6407-1e9f-4fb9-fd67-42a67d912d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "submissions_left_df = submissions_df[submissions_df['bias'] == 'left']\n",
        "submissions_right_df = submissions_df[submissions_df['bias'] == 'right']\n",
        "\n",
        "# Undersample left class in submissions to match right class\n",
        "\n",
        "right_count = len(submissions_right_df)\n",
        "submissions_left_under_df = submissions_left_df.sample(right_count)\n",
        "\n",
        "submissions_df = pd.concat([submissions_left_under_df, submissions_right_df], ignore_index=True)\n",
        "\n",
        "print(submissions_df['bias'].value_counts())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "right    434\n",
            "left     434\n",
            "Name: bias, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH9-_KrF5DPL"
      },
      "source": [
        "submission_ids = list(submissions_df['submission id'])\n",
        "\n",
        "comments_to_drop = []\n",
        "for i, row in comments_df.iterrows():\n",
        "    if row['submission id'] not in submission_ids:\n",
        "        comments_to_drop.append(i)\n",
        "\n",
        "comments_df.drop(comments_to_drop, inplace=True)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMjQe41IBRJd",
        "outputId": "825a1820-a83b-4994-e588-e6c46dab3876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(comments_df['bias'].value_counts())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "right    43569\n",
            "left      4256\n",
            "Name: bias, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obnFxRpRzlvW"
      },
      "source": [
        "### Simple text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apDuAD0hznjP"
      },
      "source": [
        "def preprocess(sentence):\n",
        "\n",
        "    # No lowercasing since upper-case words will indicate sentiment (anger or joy)\n",
        "    # Also no punctuation removal since ! and ? can indicate sentiment\n",
        "\n",
        "    # Whitespace removal\n",
        "    whitespace = '''\\n\\t'''\n",
        "\n",
        "    for ch in sentence: \n",
        "        if ch in whitespace:\n",
        "            sentence = sentence.replace(ch, \" \")\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLThuJElzwyj"
      },
      "source": [
        "submissions_df['article headline'] = submissions_df['article headline'].apply(preprocess)\n",
        "submissions_df['article body'] = submissions_df['article body'].apply(preprocess)\n",
        "comments_df['comment body'] = comments_df['comment body'].apply(preprocess)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "NWq4j_Wx0C2F",
        "outputId": "2a2b3858-da41-4006-cfe3-d46d5ee3bc5a"
      },
      "source": [
        "submissions_df"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>submission id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>Republicans now 'shocked, shocked' that there'...</td>\n",
              "      <td>© Greg Nash Republicans now 'shocked, shocked'...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jxxs8b</td>\n",
              "      <td>liberal</td>\n",
              "      <td>Georgia certifies election results confirming ...</td>\n",
              "      <td>Georgia Secretary of State Ben Raffensperger h...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kuscob</td>\n",
              "      <td>liberal</td>\n",
              "      <td>Report: QAnon Congresswoman Was Live-Tweeting ...</td>\n",
              "      <td>Domestic Terrorist: Rep. Lauren Boebert, a new...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>j2lufw</td>\n",
              "      <td>liberal</td>\n",
              "      <td>More than 175 current, former law enforcement ...</td>\n",
              "      <td>EXCLUSIVE: More than 175 current and former la...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l8m3a8</td>\n",
              "      <td>liberal</td>\n",
              "      <td>GOP group launches billboards demanding Cruz a...</td>\n",
              "      <td>GOP campaigners have called on senators Ted Cr...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>hr3aiz</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Study Shows 5.4 Million Have Lost Insurance Am...</td>\n",
              "      <td>Amid the worst public health crisis in a centu...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>lz7cve</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Bernie — also known as Mr. The Struggle Continues</td>\n",
              "      <td>We use cookies on our websites for a number of...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>jvx3os</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Medicare for All backers won in safe Democrati...</td>\n",
              "      <td>The votes were still coming in when the Democr...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>indmby</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Bernie Sanders Says Country Must Get Ready for...</td>\n",
              "      <td>Bernie Sanders is sounding the alarm. The Verm...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>le1obs</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Bernie was one of only three Senators to vote ...</td>\n",
              "      <td>Last night an amendment to keep the U.S. embas...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1039 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     submission id  ...  bias\n",
              "0           l6a0q7  ...  left\n",
              "1           jxxs8b  ...  left\n",
              "2           kuscob  ...  left\n",
              "3           j2lufw  ...  left\n",
              "4           l8m3a8  ...  left\n",
              "...            ...  ...   ...\n",
              "1034        hr3aiz  ...  left\n",
              "1035        lz7cve  ...  left\n",
              "1036        jvx3os  ...  left\n",
              "1037        indmby  ...  left\n",
              "1038        le1obs  ...  left\n",
              "\n",
              "[1039 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "is04mZWN0Dh4",
        "outputId": "c331792b-5a5c-470d-9873-e440c22790b7"
      },
      "source": [
        "comments_df"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment id</th>\n",
              "      <th>submission id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>comment body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gkzccbm</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>Hey Republican geniuses, I'll bet you were als...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gkzg91o</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>The deficit exploded after the republican tax ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gkzfown</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>The Republican Party is a fucking cancer on ou...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gkz73xz</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>I wish I had gold to give you, just for the ti...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gkzhm11</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>it's not these politicians that really bug me,...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101969</th>\n",
              "      <td>gt8z5yp</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>Non-AMP Link: [There’s a bunch of recent artic...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101970</th>\n",
              "      <td>gt9e1a4</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>The companies that don’t pay taxes (like Amazo...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101971</th>\n",
              "      <td>gt9fypm</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>“Pay taxes through stocks” is so vague as to b...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101972</th>\n",
              "      <td>gt9s4nv</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>No, he didn’t make $23B in profit. The value o...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101973</th>\n",
              "      <td>gt9w9jp</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>You also don’t lose until you sell. At least t...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101973 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       comment id  ...  bias\n",
              "0         gkzccbm  ...  left\n",
              "1         gkzg91o  ...  left\n",
              "2         gkzfown  ...  left\n",
              "3         gkz73xz  ...  left\n",
              "4         gkzhm11  ...  left\n",
              "...           ...  ...   ...\n",
              "101969    gt8z5yp  ...  left\n",
              "101970    gt9e1a4  ...  left\n",
              "101971    gt9fypm  ...  left\n",
              "101972    gt9s4nv  ...  left\n",
              "101973    gt9w9jp  ...  left\n",
              "\n",
              "[101973 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIKNDOw1z1dh"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdUAW0kN9jFi"
      },
      "source": [
        "subreddits = [\n",
        "    'liberal',\n",
        "    'democrats',\n",
        "    'conservative',\n",
        "    'republicans',\n",
        "    'obama',\n",
        "    'hillaryclinton',\n",
        "    'shitliberalssay',\n",
        "    'libertarian',\n",
        "    'sandersforpresident'\n",
        "]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5MaUvzq9ud_"
      },
      "source": [
        "# We will store only the compound (overall) sentiment\n",
        "\n",
        "results = {}\n",
        "\n",
        "for subreddit in subreddits:\n",
        "    results[subreddit] = {}\n",
        "    results[subreddit]['article headlines'] = []\n",
        "    results[subreddit]['article bodies'] = []\n",
        "    results[subreddit]['comment bodies'] = []"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZmHQjnXNwwH"
      },
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "for i, row in submissions_df.iterrows():\n",
        "    subreddit = row['subreddit']\n",
        "    headline = row['article headline']\n",
        "    body = row['article body']\n",
        "    results[subreddit]['article headlines'].append(sia.polarity_scores(headline)['compound'])\n",
        "    results[subreddit]['article bodies'].append(sia.polarity_scores(body)['compound'])\n",
        "\n",
        "for i, row in comments_df.iterrows():\n",
        "    subreddit = row['subreddit']\n",
        "    comment = row['comment body']\n",
        "    results[subreddit]['comment bodies'].append(sia.polarity_scores(comment)['compound'])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGTvQnyb_QAP",
        "outputId": "7a38efe2-5d38-4bbd-b11d-e6dd818ebd63"
      },
      "source": [
        "for subreddit in subreddits:\n",
        "    print(subreddit)\n",
        "    headline_sentiments = results[subreddit]['article headlines']\n",
        "    article_body_sentiments = results[subreddit]['article bodies']\n",
        "    comment_sentiments = results[subreddit]['comment bodies']\n",
        "    print(f\"Headline sentiment: {sum(headline_sentiments) / len(headline_sentiments)}\")\n",
        "    print(f\"Article body sentiment: {sum(article_body_sentiments) / len(article_body_sentiments)}\")\n",
        "    print(f\"Comment sentiment: {sum(comment_sentiments) / len(comment_sentiments)}\")\n",
        "    print()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "liberal\n",
            "Headline sentiment: -0.127591472868217\n",
            "Article body sentiment: -0.02572480620155042\n",
            "Comment sentiment: -0.06145019904458616\n",
            "\n",
            "democrats\n",
            "Headline sentiment: -0.16293103448275864\n",
            "Article body sentiment: 0.38970689655172425\n",
            "Comment sentiment: 0.03598296022201663\n",
            "\n",
            "conservative\n",
            "Headline sentiment: -0.05971562500000001\n",
            "Article body sentiment: -4.374999999996743e-06\n",
            "Comment sentiment: 0.019022813248099398\n",
            "\n",
            "republicans\n",
            "Headline sentiment: -0.02522911392405063\n",
            "Article body sentiment: 0.1171746835443038\n",
            "Comment sentiment: -0.06247160493827159\n",
            "\n",
            "obama\n",
            "Headline sentiment: 0.10161307692307692\n",
            "Article body sentiment: 0.5443384615384614\n",
            "Comment sentiment: 0.30162500000000014\n",
            "\n",
            "hillaryclinton\n",
            "Headline sentiment: 0.03828181818181818\n",
            "Article body sentiment: 0.5253062937062937\n",
            "Comment sentiment: 0.0345304347826087\n",
            "\n",
            "shitliberalssay\n",
            "Headline sentiment: -0.41447500000000004\n",
            "Article body sentiment: -0.33492500000000003\n",
            "Comment sentiment: -0.015377812853373501\n",
            "\n",
            "libertarian\n",
            "Headline sentiment: -0.096634554973822\n",
            "Article body sentiment: -0.032361256544502566\n",
            "Comment sentiment: -0.05258004820699454\n",
            "\n",
            "sandersforpresident\n",
            "Headline sentiment: 0.050557777777777764\n",
            "Article body sentiment: 0.2352311111111111\n",
            "Comment sentiment: 0.061219410160734664\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yCY8lX1NrGy"
      },
      "source": [
        "### Further preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUm1l8L6R2o_"
      },
      "source": [
        "Now we perform further text preprocessing before vocab analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5igJsQRz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a019b1-e21c-4da6-ad77-9b89f88f2b9d"
      },
      "source": [
        "# Text preprocessing preparation\n",
        "\n",
        "#stop_words = [\"the\", \"a\", \"an\", \"as\", \"this\", \"that\", \"is\", \"and\", \"or\", \"on\",\n",
        "#              \"at\", \"to\", \"in\", \"by\", \"than\", \"of\", \"for\", \"be\", \"i\", \"you\", \n",
        "#              \"he\", \"she\", \"his\", \"her\", \"do\", \"it\", \"with\"]\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# required for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# required for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zeni9Z_1NwbL"
      },
      "source": [
        "def preprocess(sentence):\n",
        "\n",
        "    # Lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Punctuation removal\n",
        "    punctuations = '''!()-—[]{};:'\"“”‘’\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "    for ch in sentence: \n",
        "        if ch in punctuations: \n",
        "            sentence = sentence.replace(ch, \"\")\n",
        "\n",
        "    # Stop word removal\n",
        "    remaining_words = [word for word in sentence.split() if not word in stopwords.words()]\n",
        "\n",
        "    sentence = \" \".join(remaining_words)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = []\n",
        "\n",
        "    # In order to lemmatise we must first POS-tag each sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    for word, tag in tagged:\n",
        "        pos = nltk_tag_to_wordnet_tag(tag) \n",
        "        if pos is not None:\n",
        "            word = lemmatizer.lemmatize(word, pos=pos)\n",
        "\n",
        "        lemmatized_words.append(word)\n",
        "\n",
        "    sentence = \" \".join(lemmatized_words)\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avRNm30BS7eM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "2753f3e6-3707-4a2c-8a6b-dcd13c05b6ee"
      },
      "source": [
        "submissions_df['article headline'] = submissions_df['article headline'].apply(preprocess)\n",
        "submissions_df['article body'] = submissions_df['article body'].apply(preprocess)\n",
        "comments_df['comment body'] = comments_df['comment body'].apply(preprocess)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-694438922d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubmissions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article headline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubmissions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcomments_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-87a665c346e1>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Stop word removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mremaining_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-87a665c346e1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Stop word removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mremaining_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hv3-AEuuR7u"
      },
      "source": [
        "submissions_df.to_csv('submissions_preprocessed.tsv', sep='\\t', index=False)\n",
        "comments_df.to_csv('comments_preprocessed.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu2eTdaSTFHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "89f9de45-0c97-43a4-f75f-2caaa9fdcb09"
      },
      "source": [
        "submissions_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>submission id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>article headline</th>\n",
              "      <th>article body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>republican now shock shocked there deficit hah...</td>\n",
              "      <td>© greg nash republican now shock shocked there...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>jxxs8b</td>\n",
              "      <td>liberal</td>\n",
              "      <td>georgia certifies election result confirm bide...</td>\n",
              "      <td>georgia secretary state ben raffensperger hold...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kuscob</td>\n",
              "      <td>liberal</td>\n",
              "      <td>report qanon congresswoman be livetweeting nan...</td>\n",
              "      <td>domestic terrorist rep lauren boebert newly el...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>j2lufw</td>\n",
              "      <td>liberal</td>\n",
              "      <td>more 175 current former law enforcement offici...</td>\n",
              "      <td>exclusive more 175 current former law enforcem...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>l8m3a8</td>\n",
              "      <td>liberal</td>\n",
              "      <td>gop group launch billboard demand cruz hawley ...</td>\n",
              "      <td>gop campaigner have call senator ted cruz josh...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>hr3aiz</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>study show 54 million have lose insurance amid...</td>\n",
              "      <td>amid bad public health crisis century devastat...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>lz7cve</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>bernie also know mr struggle continue</td>\n",
              "      <td>we use cooky our website number purpose includ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>jvx3os</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>medicare all backer win safe democratic distri...</td>\n",
              "      <td>vote be still come when democratic establishme...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>indmby</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>bernie sander say country must get ready trump...</td>\n",
              "      <td>bernie sander sound alarm vermont senator warn...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>le1obs</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>bernie be one only three senator vote against ...</td>\n",
              "      <td>last night amendment keep us embassy jerusalem...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1039 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     submission id  ...  bias\n",
              "0           l6a0q7  ...  left\n",
              "1           jxxs8b  ...  left\n",
              "2           kuscob  ...  left\n",
              "3           j2lufw  ...  left\n",
              "4           l8m3a8  ...  left\n",
              "...            ...  ...   ...\n",
              "1034        hr3aiz  ...  left\n",
              "1035        lz7cve  ...  left\n",
              "1036        jvx3os  ...  left\n",
              "1037        indmby  ...  left\n",
              "1038        le1obs  ...  left\n",
              "\n",
              "[1039 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhz2MTlDTHNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1f9dd6fb-e511-4fdf-a62c-49f125b37ee8"
      },
      "source": [
        "comments_df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment id</th>\n",
              "      <th>submission id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>comment body</th>\n",
              "      <th>bias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gkzccbm</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>hey republican genius ill bet be also unaware ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gkzg91o</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>deficit explode after republican tax cut not s...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gkzfown</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>republican party fuck cancer our country they ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gkz73xz</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>wish have gold give just title alone</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gkzhm11</td>\n",
              "      <td>l6a0q7</td>\n",
              "      <td>liberal</td>\n",
              "      <td>its not these politician really bug me their r...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101969</th>\n",
              "      <td>gt8z5yp</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>nonamp link there bunch recent article out the...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101970</th>\n",
              "      <td>gt9e1a4</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>company dont pay tax like amazon nike obvious ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101971</th>\n",
              "      <td>gt9fypm</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>pay tax through stock so vague meaningless hed...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101972</th>\n",
              "      <td>gt9s4nv</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>no didnt make 23b profit value share increase ...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101973</th>\n",
              "      <td>gt9w9jp</td>\n",
              "      <td>mj44yw</td>\n",
              "      <td>sandersforpresident</td>\n",
              "      <td>also dont lose until sell least thats what tel...</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101973 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       comment id  ...  bias\n",
              "0         gkzccbm  ...  left\n",
              "1         gkzg91o  ...  left\n",
              "2         gkzfown  ...  left\n",
              "3         gkz73xz  ...  left\n",
              "4         gkzhm11  ...  left\n",
              "...           ...  ...   ...\n",
              "101969    gt8z5yp  ...  left\n",
              "101970    gt9e1a4  ...  left\n",
              "101971    gt9fypm  ...  left\n",
              "101972    gt9s4nv  ...  left\n",
              "101973    gt9w9jp  ...  left\n",
              "\n",
              "[101973 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNXujLRcM_3W"
      },
      "source": [
        "### Vocab overlap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6xDSDEBNAt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbec6493-9708-4ea0-d1d6-796d94f1fbcd"
      },
      "source": [
        "article_headline_vocab = []\n",
        "article_body_vocab = []\n",
        "comment_vocab = []\n",
        "\n",
        "for i, row in submissions_df.iterrows():\n",
        "    article_headline = row['article headline']\n",
        "    article_body = row['article body']\n",
        "    for word in article_headline.split():\n",
        "        article_headline_vocab.append(word)\n",
        "    for word in article_body.split():\n",
        "        article_body_vocab.append(word)\n",
        "\n",
        "for i, row in comments_df.iterrows():\n",
        "    comment_body = row['comment body']\n",
        "    for word in comment_body.split():\n",
        "        comment_vocab.append(word)\n",
        "\n",
        "print(len(article_headline_vocab))\n",
        "print(len(article_body_vocab))\n",
        "print(len(comment_vocab))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13062\n",
            "418125\n",
            "2345412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYTHcILBNmdr"
      },
      "source": [
        "article_headline_multiset = Counter(article_headline_vocab)\n",
        "article_body_multiset = Counter(article_body_vocab)\n",
        "comment_multiset = Counter(comment_vocab)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AINbnyPRe3rQ",
        "outputId": "b0a24b78-2531-4916-b1d8-cf74985788f5"
      },
      "source": [
        "# Computing Jaccard distances\n",
        "\n",
        "headline_body_intersect = list((article_headline_multiset & article_body_multiset).elements())\n",
        "headline_comment_intersect = list((article_headline_multiset & comment_multiset).elements())\n",
        "body_comment_intersect = list((article_body_multiset & comment_multiset).elements())\n",
        "\n",
        "print(len(headline_body_intersect))\n",
        "print(len(headline_comment_intersect))\n",
        "print(len(body_comment_intersect))\n",
        "\n",
        "headline_body_union = list((article_headline_multiset | article_body_multiset).elements())\n",
        "headline_comment_union = list((article_headline_multiset | comment_multiset).elements())\n",
        "body_comment_union = list((article_body_multiset | comment_multiset).elements())\n",
        "\n",
        "print(len(headline_body_union))\n",
        "print(len(headline_comment_union))\n",
        "print(len(body_comment_union))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12775\n",
            "12804\n",
            "386738\n",
            "418412\n",
            "2345670\n",
            "2376799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zahTnTbIfeiQ",
        "outputId": "46b7bc5e-82fc-45d5-8483-7cd0ea3a5c7a"
      },
      "source": [
        "headline_body_jaccard = len(headline_body_intersect) / len(headline_body_union)\n",
        "headline_comment_jaccard = len(headline_comment_intersect) / len(headline_comment_union)\n",
        "body_comment_jaccard = len(body_comment_intersect) / len(body_comment_union)\n",
        "\n",
        "print(headline_body_jaccard)\n",
        "print(headline_comment_jaccard)\n",
        "print(body_comment_jaccard)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.030532107109738728\n",
            "0.005458568340815204\n",
            "0.1627138012091052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7rpzfpkhx_M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}