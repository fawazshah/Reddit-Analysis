{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_direct_transfer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpj29EkexQElhhOyUBbgti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fawazshah/Reddit-Analysis/blob/main/5_direct_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91kZ2LAThNyT",
        "outputId": "72c46efd-2638-4abf-a895-c6535b8bbd44"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MYNYLDSz3nw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "import time\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dGmQ_vZhc7u"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYWywz7Ghdc7"
      },
      "source": [
        "### BERT setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4XGy9Ehg1l"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PupORWmFhhLN"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODYBT_D-hkCF"
      },
      "source": [
        "# Compute the length of the longest sentence in particular column out of\n",
        "# all train, val and test data\n",
        "def compute_max_length(df, bert_input_func):\n",
        "\n",
        "  sentences = bert_input_func(df)\n",
        "\n",
        "  max_len = 0\n",
        "\n",
        "  for sent in sentences:\n",
        "\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "      # Update the maximum sentence length.\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "\n",
        "  return max_len"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnaBAsRehouy"
      },
      "source": [
        "def compute_sentences_article_body(df):\n",
        "    return list(df['article body'])\n",
        "\n",
        "def compute_sentences_comment_body(df):\n",
        "    return list(df['comment body'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLj7g6-l7O4"
      },
      "source": [
        "def create_bert_dataset(df, bert_input_func, max_sequence_len):\n",
        "    # Returns a TensorDataset of sequences extracted from df\n",
        "\n",
        "    token_ids = []\n",
        "    token_type_ids = [] # segment ids \n",
        "    attention_masks = []\n",
        "\n",
        "    sentences = bert_input_func(df)\n",
        "\n",
        "    for sent in sentences:\n",
        "        encoding_dict = tokenizer(sent,\n",
        "                                  add_special_tokens=True,\n",
        "                                  max_length=max_sequence_len,\n",
        "                                  padding='max_length',\n",
        "                                  truncation=True,\n",
        "                                  return_token_type_ids = True,\n",
        "                                  return_attention_mask = True,\n",
        "                                  return_tensors = 'pt'\n",
        "                                  )\n",
        "        token_ids.append(encoding_dict['input_ids'])\n",
        "        token_type_ids.append(encoding_dict['token_type_ids'])\n",
        "        attention_masks.append(encoding_dict['attention_mask'])\n",
        "    \n",
        "    token_ids = torch.cat(token_ids, dim=0)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(df['bias'].values)\n",
        "    \n",
        "    return TensorDataset(token_ids, token_type_ids, attention_masks, labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIZt1ovhl8gs"
      },
      "source": [
        "def train_BERT(train_dataloader, val_dataloader, model, number_epoch):\n",
        "\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5, \n",
        "                    eps = 1e-8 \n",
        "                )\n",
        "\n",
        "    # Create the learning rate scheduler.\n",
        "    total_steps = len(train_dataloader) * number_epoch\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
        "                                                num_training_steps=total_steps)\n",
        "\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "\n",
        "        # TRAINING\n",
        "\n",
        "        time0 = time.time()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        epoch_train_loss = 0\n",
        "        no_observations = 0\n",
        "        epoch_train_predictions = []\n",
        "        epoch_train_labels = []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "\n",
        "            # Each batch contains token ids, token type ids, attention masks and labels\n",
        "            b_token_ids = batch[0].to(device)\n",
        "            b_token_type_ids = batch[1].to(device)\n",
        "            b_attention_masks = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            no_observations = no_observations + b_labels.shape[0]\n",
        "            \n",
        "            output = model(b_token_ids, \n",
        "                    token_type_ids=b_token_type_ids, \n",
        "                    attention_mask=b_attention_masks, \n",
        "                    labels=b_labels)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss = output.loss\n",
        "            logits = output.logits\n",
        "\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = b_labels.detach().cpu().numpy()\n",
        "            epoch_train_predictions.extend(predictions)\n",
        "            epoch_train_labels.extend(labels)\n",
        "\n",
        "            loss.backward()\n",
        "            # Clip the norm of the gradients to 1 to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step() \n",
        "\n",
        "            # Update the learning rate using the scheduler\n",
        "            scheduler.step()  \n",
        "\n",
        "            epoch_train_loss += loss.item()*b_labels.shape[0]\n",
        "\n",
        "        epoch_train_loss, epoch_train_acc = epoch_train_loss / no_observations, accuracy_score(epoch_train_labels, epoch_train_predictions)\n",
        "\n",
        "        # VALIDATION\n",
        "\n",
        "        epoch_valid_loss, epoch_val_predictions, epoch_val_labels = evaluate_BERT(val_dataloader, model)\n",
        "        epoch_valid_acc = accuracy_score(epoch_val_labels, epoch_val_predictions)\n",
        "\n",
        "        # FINALLY\n",
        "\n",
        "        print(f\"Epoch took: {time.time() - time0}\")\n",
        "\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_train_loss:.2f} | Train Accuracy: {epoch_train_acc:.2f} | \\\n",
        "        Val. Loss: {epoch_valid_loss:.2f} | Val. Accuracy: {epoch_valid_acc:.2f} |')\n",
        "\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "    \n",
        "    return train_loss, valid_loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUmDxCEl-oY"
      },
      "source": [
        "def evaluate_BERT(test_dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    no_observations = 0\n",
        "    predictions_all = []\n",
        "    labels_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            b_token_ids = batch[0].to(device)\n",
        "            b_token_type_ids = batch[1].to(device)\n",
        "            b_attention_masks = batch[2].to(device)\n",
        "            b_labels = batch[3].to(device)\n",
        "\n",
        "            no_observations += b_labels.shape[0]\n",
        "            output = model(b_token_ids, token_type_ids=b_token_type_ids, \n",
        "                                        attention_mask=b_attention_masks)\n",
        "            logits = output.logits\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "\n",
        "            predictions = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = b_labels.detach().cpu().numpy()\n",
        "            predictions_all.extend(predictions)\n",
        "            labels_all.extend(labels)\n",
        "\n",
        "            total_loss += loss.item()*b_labels.shape[0]\n",
        "    \n",
        "    return total_loss / no_observations, predictions_all, labels_all"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imTptCu8ZTQc"
      },
      "source": [
        "### Article body -> comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBttHU80AyR"
      },
      "source": [
        "#### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAFuwYtc0Ewr"
      },
      "source": [
        "submissions_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/collated-data/submissions_preprocessed.tsv'\n",
        "submissions_df = pd.read_csv(submissions_url, sep='\\t')\n",
        "\n",
        "comments_url = 'https://raw.githubusercontent.com/fawazshah/Reddit-Analysis/master/data/collated-data/comments_preprocessed.tsv'\n",
        "comments_df = pd.read_csv(comments_url, sep='\\t')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnI9nKnd0GTu",
        "outputId": "f365456c-b53e-4b03-f92a-c296f13dd09c"
      },
      "source": [
        "print(f\"No. submissions: {len(submissions_df)}\")\n",
        "print(f\"No. comments: {len(comments_df)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. submissions: 806\n",
            "No. comments: 11923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpYGMib_rdRw"
      },
      "source": [
        "#### One hot encode labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM5LgLVrrj67"
      },
      "source": [
        "# left == 0\n",
        "# right == 1\n",
        "\n",
        "def encode_labels(label):\n",
        "    if label == \"left\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "submissions_df['bias'] = submissions_df['bias'].apply(encode_labels)\n",
        "comments_df['bias'] = comments_df['bias'].apply(encode_labels)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vEbHBzSfbMk"
      },
      "source": [
        "#### Split data into train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8myfRK95Dh9"
      },
      "source": [
        "# Train/val/test split\n",
        "\n",
        "TRAIN = 0.7\n",
        "VAL = 0.1\n",
        "TEST = 0.2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbgYYN43ficb"
      },
      "source": [
        "split_point_1 = int(TRAIN*len(submissions_df))\n",
        "split_point_2 = int((TRAIN+VAL)*len(submissions_df))\n",
        "\n",
        "submissions_train_df = submissions_df.iloc[:split_point_1].copy()\n",
        "submissions_val_df = submissions_df.iloc[split_point_1:split_point_2].copy()\n",
        "submissions_test_df = submissions_df.iloc[split_point_2:].copy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IScJW1Tbf2-R",
        "outputId": "3acd956a-ee36-41ba-9476-06cbb8291e00"
      },
      "source": [
        "print(f\"Size of training set: {len(submissions_train_df)}\")\n",
        "print(f\"Size of validation set: {len(submissions_val_df)}\")\n",
        "print(f\"Size of test set: {len(submissions_test_df)}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 564\n",
            "Size of validation set: 80\n",
            "Size of test set: 162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_GyU8XqmR0c"
      },
      "source": [
        "#### Create BERT dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSekzkAXf5VK"
      },
      "source": [
        "dataloaders = {}\n",
        "BATCH_SIZE = 10"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEdV5-w5mVoh"
      },
      "source": [
        "# Using no folds\n",
        "dataloaders['article bodies'] = {}\n",
        "train_dataset = create_bert_dataset(submissions_train_df, compute_sentences_article_body, 512)\n",
        "dataloaders['article bodies']['train'] = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE)\n",
        "val_dataset = create_bert_dataset(submissions_val_df, compute_sentences_article_body, 512)\n",
        "dataloaders['article bodies']['val'] = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=BATCH_SIZE)\n",
        "test_dataset = create_bert_dataset(submissions_test_df, compute_sentences_article_body, 512)\n",
        "dataloaders['article bodies']['test'] = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPSiBWRgq1sj"
      },
      "source": [
        "comment_dataset = create_bert_dataset(comments_df, compute_sentences_comment_body, 512)\n",
        "dataloaders['comment bodies'] = DataLoader(comment_dataset, sampler = RandomSampler(comment_dataset), batch_size=BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abau8yBniPJ8"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}